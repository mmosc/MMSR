{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import exists\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import datatable as dt\n",
    "# Variables that contains the file location\n",
    "from files import *\n",
    "from functions import *\n",
    "from sklearn.preprocessing import normalize\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datatype used to reduce memory required\n",
    "DTYPE = np.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string ids are inefficient, let's use integers and a lookup table\n",
    "def getRelationIdsToNumbers(df):\n",
    "    no_ids = len(df.index.values)\n",
    "    return dict(zip(df.index.values, list(range(no_ids)))), np.arange(no_ids)\n",
    "\n",
    "def change_id_to_keys(df, id_to_keys):\n",
    "    df.set_index(np.asarray([id_to_keys[i] for i in df.index.values]), inplace=True)\n",
    "    df.index.astype(np.int32, copy=False)\n",
    "    df.index.name = \"id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save memory we converted the original indices to new indices with only numbers.\n",
    "# The following function create those ids or if they already exists loads them\n",
    "# The ids have to be similar in all dataframes to have consistency, \n",
    "# for that we use the info indexes as base for all dataframes\n",
    "\n",
    "file_original_new_ids = \"./data/relation_original_new_ids.csv\"\n",
    "\n",
    "info  = dt.fread(file_info_2).to_pandas().set_index('id')\n",
    "id_to_key, indexes = getRelationIdsToNumbers(info)\n",
    "\n",
    "\n",
    "if exists(file_original_new_ids):\n",
    "    relation_ids = pd.read_csv(file_original_new_ids).set_index('original').astype(DTYPE)\n",
    "else:\n",
    "    \n",
    "    relation_ids = pd.DataFrame(columns=['original', 'newId'])\n",
    "    relation_ids['original'] = list(id_to_key.keys())\n",
    "    relation_ids['newId'] = list(id_to_key.values())\n",
    "    relation_ids.set_index('original', inplace=True)\n",
    "    relation_ids = relation_ids.astype(DTYPE)\n",
    "    relation_ids.to_csv(file_original_new_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>newId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0009fFIM1eYThaPg</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0010xmHR6UICBOYT</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  newId\n",
       "original               \n",
       "0009fFIM1eYThaPg    0.0\n",
       "0010xmHR6UICBOYT    1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Relation of original id, to new id\n",
    "relation_ids.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets\n",
    "\n",
    "Now after having a relation of original ids to new ids, we can load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres  = dt.fread(file_genres_2).to_pandas().set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf_idf   = dt.fread(file_tfidf_2 ,  header=True).to_pandas().set_index('id').astype(DTYPE)\n",
    "word2vec = dt.fread(file_word2vec_2, header=True).to_pandas().set_index('id').astype(DTYPE)\n",
    "bert     = dt.fread(file_bert_2, header=True).to_pandas().set_index('id').astype(DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "blf_correlation      = dt.fread(file_blf_correlation, header=True).to_pandas().set_index('id').astype(DTYPE)\n",
    "blf_deltaspectral    = dt.fread(file_blf_deltaspectral, header=True).to_pandas().set_index('id').astype(DTYPE)\n",
    "blf_spectral         = dt.fread(file_blf_spectral, header=True).to_pandas().set_index('id').astype(DTYPE)\n",
    "blf_spectralcontrast = dt.fread(file_blf_spectralcontrast, header=True).to_pandas().set_index('id').astype(DTYPE)\n",
    "blf_vardeltaspectral = dt.fread(file_blf_vardeltaspectral, header=True).to_pandas().set_index('id').astype(DTYPE)\n",
    "essentia             = dt.fread(file_essentia, header=True).to_pandas().set_index('id').astype(DTYPE)\n",
    "mfcc_bow             = dt.fread(file_mfcc_bow, header=True).to_pandas().set_index('id').astype(DTYPE)\n",
    "mfcc_stats           = dt.fread(file_mfcc_stats, header=True).to_pandas().set_index('id').astype(DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "blf_logfluc  = dt.fread(file_blf_logfluc)\n",
    "# This is done because in the csv it has an extra column name, \n",
    "# so in case someone with the original dataset tries to run it, it fixes that error\n",
    "# It looks weird, but it is because first i am loading the data into datatable and then pass it to dataframe\n",
    "new_cols = ['id']\n",
    "new_cols.extend(list(blf_logfluc.names[2:]))\n",
    "new_cols = tuple(new_cols)\n",
    "del blf_logfluc[:, -1]\n",
    "\n",
    "blf_logfluc.names = new_cols\n",
    "blf_logfluc = blf_logfluc.to_pandas()\n",
    "blf_logfluc.set_index('id', inplace=True)\n",
    "blf_logfluc = blf_logfluc.astype(DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "incp   = dt.fread(file_incp , header=True).to_pandas().set_index('id').astype(DTYPE)\n",
    "resnet = dt.fread(file_resnet, header=True).to_pandas().set_index('id').astype(DTYPE)\n",
    "vgg19  = dt.fread(file_vgg19, header=True).to_pandas().set_index('id').astype(DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"tfidf\" : tf_idf,\n",
    "    \"word2vec\" : word2vec,\n",
    "    \"bert\" : bert,\n",
    "    \"mfcc_bow\" : mfcc_bow,\n",
    "    \"mfcc_stats\" : mfcc_stats,\n",
    "    \"essentia\" : essentia,\n",
    "    \"blf_delta_spectral\" : blf_deltaspectral,\n",
    "    \"blf_correlation\" : blf_correlation,\n",
    "    \"blf_logfluc\" : blf_logfluc,\n",
    "    \"blf_spectral\" : blf_spectral,\n",
    "    \"blf_spectral_contrast\" : blf_spectralcontrast,\n",
    "    \"blf_vardelta_spectral\" : blf_vardeltaspectral,\n",
    "    \"incp\" : incp,\n",
    "    \"vgg19\" : vgg19,\n",
    "    \"resnet\" : resnet\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change the original ids to the new ids of the loaded datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_id_to_keys(genres,id_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in datasets.keys():\n",
    "    change_id_to_keys(datasets[df], id_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature tfidf (68641, 1000)\n",
      "Feature word2vec (68641, 300)\n",
      "Feature bert (68641, 768)\n",
      "Feature mfcc_bow (68641, 500)\n",
      "Feature mfcc_stats (68641, 104)\n",
      "Feature essentia (68641, 1034)\n",
      "Feature blf_delta_spectral (68641, 1372)\n",
      "Feature blf_correlation (68641, 1326)\n",
      "Feature blf_logfluc (68641, 3626)\n",
      "Feature blf_spectral (68641, 980)\n",
      "Feature blf_spectral_contrast (68641, 800)\n",
      "Feature blf_vardelta_spectral (68641, 1344)\n",
      "Feature incp (68641, 4096)\n",
      "Feature vgg19 (68641, 8192)\n",
      "Feature resnet (68641, 4096)\n"
     ]
    }
   ],
   "source": [
    "for df in datasets.keys():\n",
    "    print(f\"Feature {df}\" , datasets[df].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to compute distance and only retrieve the top ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_top(data: np.array, simfunction, idx_values, batches:int = 1, top: int=100):\n",
    "    \n",
    "    splits = np.array_split(data, batches, axis=0)\n",
    "    splits_idx = np.array_split(np.arange(data.shape[0]), batches, axis=0)\n",
    "\n",
    "    top_values = np.zeros((data.shape[0], top), dtype=np.int32)\n",
    "    for b,i in tqdm(list(zip(splits, splits_idx))):\n",
    "        size_batch = b.shape[0]\n",
    "        \n",
    "        ### Calculate similarities\n",
    "        results = simfunction(data, b).T\n",
    "        \n",
    "        ### Get Tops\n",
    "        # Set the distance to the same document to -1 because we dont want it at the start.\n",
    "        results[(np.arange(size_batch),i)] = -1\n",
    "        # Get the document indices instead of the distances\n",
    "        top_values[i, :] =(idx_values[np.argsort(results * -1, axis=1)][:,:top])\n",
    "\n",
    "    return top_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example on how to use the function\n",
    "# topExample = compute_top(\n",
    "#     tf_idf.to_numpy(dtype=DTYPE),\n",
    "#     get_cosine_similarity,\n",
    "#     tf_idf.index.values,\n",
    "#     batches=100,\n",
    "#     top=100\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of Top 100 ids with different feature vectors cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/top_ids_cosine_tfidf.csv\n",
      "./data/top_ids_cosine_word2vec.csv\n",
      "./data/top_ids_cosine_bert.csv\n",
      "./data/top_ids_cosine_mfcc_bow.csv\n",
      "./data/top_ids_cosine_mfcc_stats.csv\n",
      "./data/top_ids_cosine_essentia.csv\n",
      "./data/top_ids_cosine_blf_delta_spectral.csv\n",
      "./data/top_ids_cosine_blf_correlation.csv\n",
      "./data/top_ids_cosine_blf_logfluc.csv\n",
      "./data/top_ids_cosine_blf_spectral.csv\n",
      "./data/top_ids_cosine_blf_spectral_contrast.csv\n",
      "./data/top_ids_cosine_blf_vardelta_spectral.csv\n",
      "./data/top_ids_cosine_incp.csv\n",
      "./data/top_ids_cosine_vgg19.csv\n",
      "./data/top_ids_cosine_resnet.csv\n"
     ]
    }
   ],
   "source": [
    "features = datasets.keys() \n",
    "for feature in features:\n",
    "    file_name = f'./data/top_ids_cosine_{feature}.csv'\n",
    "    print(file_name)\n",
    "#Uncomment to generate ids\n",
    "#     topData = compute_top(\n",
    "#         datasets[feature].to_numpy(dtype=DTYPE), \n",
    "#         get_cosine_similarity,\n",
    "#         datasets[feature].index.values,\n",
    "#         batches=100,\n",
    "#         top=100\n",
    "#     )\n",
    "#     dt.Frame(pd.DataFrame(topData, index=datasets[feature].index.values).reset_index()).to_csv(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOP 100 Ids using each song as a query with jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/top_ids_jaccard_tfidf.csv\n",
      "./data/top_ids_jaccard_word2vec.csv\n",
      "./data/top_ids_jaccard_bert.csv\n",
      "./data/top_ids_jaccard_mfcc_bow.csv\n",
      "./data/top_ids_jaccard_mfcc_stats.csv\n",
      "./data/top_ids_jaccard_essentia.csv\n",
      "./data/top_ids_jaccard_blf_delta_spectral.csv\n",
      "./data/top_ids_jaccard_blf_correlation.csv\n",
      "./data/top_ids_jaccard_blf_logfluc.csv\n",
      "./data/top_ids_jaccard_blf_spectral.csv\n",
      "./data/top_ids_jaccard_blf_spectral_contrast.csv\n",
      "./data/top_ids_jaccard_blf_vardelta_spectral.csv\n",
      "./data/top_ids_jaccard_incp.csv\n",
      "./data/top_ids_jaccard_vgg19.csv\n",
      "./data/top_ids_jaccard_resnet.csv\n"
     ]
    }
   ],
   "source": [
    "features = datasets.keys() \n",
    "for feature in features:\n",
    "    file_name = f'./data/top_ids_jaccard_{feature}.csv'\n",
    "    print(file_name)\n",
    "#Uncomment to generate ids\n",
    "#     topData = compute_top(\n",
    "#         datasets[feature].to_numpy(dtype=DTYPE), \n",
    "#         get_jaccard_similarity,\n",
    "#         datasets[feature].index.values,\n",
    "#         batches=100,\n",
    "#         top=100\n",
    "#     )\n",
    "#     dt.Frame(pd.DataFrame(topData, index=datasets[feature].index.values).reset_index()).to_csv(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Baseline\n",
    "\n",
    "The baseline that we took in consideration is a random selection of songs without repeting the ids for the same query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68641, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 68641/68641 [01:02<00:00, 1097.35it/s]\n"
     ]
    }
   ],
   "source": [
    "top_random_ids = np.empty((68641, 100), dtype=np.int32)\n",
    "print(top_random_ids.shape)\n",
    "np.random.seed(42)\n",
    "for i in tqdm(range(68641)):\n",
    "    top_random_ids[i] = np.random.choice(68641, 100,replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([51329, 19980, 15110, 63047,  9736], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_random_ids[0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68641, 100)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_random_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.Frame(pd.DataFrame(top_random_ids, index=list(range(68641)) ).reset_index()).to_csv('./data/top_ids_baseline.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_topids = np.take(tf_idf.index.values, top_random_ids.astype(int), axis=0)\n",
    "# baseline_topids[0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt.Frame(pd.DataFrame(baseline_topids, index=tf_idf.index.values ).reset_index()).to_csv('./TopIdsTask2/top_ids_baseline_complete.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early fusion datasets combining Lyrics Audio Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computations concatenating each dataset and then compute the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_jaccard = [\"essentia\", \"blf_logfluc\", \"mfcc_stats\"]\n",
    "lyrics_jaccard = [\"bert\", \"tfidf\"]\n",
    "video_jaccard = [\"vgg19\", \"resnet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/top_ids_jaccard_earlyfusion_bert_essentia_vgg19.csv\n",
      "./data/top_ids_jaccard_earlyfusion_bert_essentia_resnet.csv\n",
      "./data/top_ids_jaccard_earlyfusion_bert_blf_logfluc_vgg19.csv\n",
      "./data/top_ids_jaccard_earlyfusion_bert_blf_logfluc_resnet.csv\n",
      "./data/top_ids_jaccard_earlyfusion_bert_mfcc_stats_vgg19.csv\n",
      "./data/top_ids_jaccard_earlyfusion_bert_mfcc_stats_resnet.csv\n",
      "./data/top_ids_jaccard_earlyfusion_tfidf_essentia_vgg19.csv\n",
      "./data/top_ids_jaccard_earlyfusion_tfidf_essentia_resnet.csv\n",
      "./data/top_ids_jaccard_earlyfusion_tfidf_blf_logfluc_vgg19.csv\n",
      "./data/top_ids_jaccard_earlyfusion_tfidf_blf_logfluc_resnet.csv\n",
      "./data/top_ids_jaccard_earlyfusion_tfidf_mfcc_stats_vgg19.csv\n",
      "./data/top_ids_jaccard_earlyfusion_tfidf_mfcc_stats_resnet.csv\n"
     ]
    }
   ],
   "source": [
    "for lyrics in lyrics_jaccard:\n",
    "    for audio in audio_jaccard:\n",
    "        for video in video_jaccard:\n",
    "            file_name = f'./data/top_ids_jaccard_earlyfusion_{lyrics}_{audio}_{video}.csv'\n",
    "            print(file_name)\n",
    "#             index_values = datasets[lyrics].index.values\n",
    "#             topData = compute_top(\n",
    "#                 datasets[lyrics].join(datasets[audio], on='id').join(datasets[video], on=\"id\").to_numpy(dtype=DTYPE),\n",
    "#                 get_jaccard_similarity,\n",
    "#                 index_values,\n",
    "#                 batches=100,\n",
    "#                 top=100\n",
    "#             )\n",
    "#             dt.Frame(pd.DataFrame(topData, index=index_values).reset_index()).to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_cosine = [\"blf_spectral\", \"blf_logfluc\", \"mfcc_bow\"]\n",
    "lyrics_cosine = [\"bert\", \"tfidf\"]\n",
    "video_cosine = [\"incp\", \"resnet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/top_ids_cosine_earlyfusion_bert_blf_spectral_incp.csv\n",
      "./data/top_ids_cosine_earlyfusion_bert_blf_spectral_resnet.csv\n",
      "./data/top_ids_cosine_earlyfusion_bert_blf_logfluc_incp.csv\n",
      "./data/top_ids_cosine_earlyfusion_bert_blf_logfluc_resnet.csv\n",
      "./data/top_ids_cosine_earlyfusion_bert_mfcc_bow_incp.csv\n",
      "./data/top_ids_cosine_earlyfusion_bert_mfcc_bow_resnet.csv\n",
      "./data/top_ids_cosine_earlyfusion_tfidf_blf_spectral_incp.csv\n",
      "./data/top_ids_cosine_earlyfusion_tfidf_blf_spectral_resnet.csv\n",
      "./data/top_ids_cosine_earlyfusion_tfidf_blf_logfluc_incp.csv\n",
      "./data/top_ids_cosine_earlyfusion_tfidf_blf_logfluc_resnet.csv\n",
      "./data/top_ids_cosine_earlyfusion_tfidf_mfcc_bow_incp.csv\n",
      "./data/top_ids_cosine_earlyfusion_tfidf_mfcc_bow_resnet.csv\n"
     ]
    }
   ],
   "source": [
    "for lyrics in lyrics_cosine:\n",
    "    for audio in audio_cosine:\n",
    "        for video in video_cosine:\n",
    "            file_name = f'./data/top_ids_cosine_earlyfusion_{lyrics}_{audio}_{video}.csv'\n",
    "            print(file_name)\n",
    "#             index_values = datasets[lyrics].index.values\n",
    "#             topData = compute_top(\n",
    "#                 datasets[lyrics].join(datasets[audio], on='id').join(datasets[video], on=\"id\").to_numpy(dtype=DTYPE),\n",
    "#                 get_cosine_similarity,\n",
    "#                 index_values,\n",
    "#                 batches=100,\n",
    "#                 top=100\n",
    "#             )\n",
    "#             dt.Frame(pd.DataFrame(topData, index=index_values).reset_index()).to_csv(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Late fusion only combine 3 features wit Borda Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_vote_values(a):\n",
    "    return dict([(vote, len(a) - i) for i, vote in enumerate(a)])\n",
    "\n",
    "\n",
    "def accumulate_votes(votes1,votes2,votes3):\n",
    "    votesAccumulated = []\n",
    "    for idx, query in enumerate(votes1):\n",
    "        votesByQuery = {}\n",
    "        keys = votes1[idx].keys()\n",
    "        for key in keys:\n",
    "            votesByQuery[key] = votes1[idx][key] + votes2[idx][key] + votes3[idx][key]\n",
    "        sorted_votes = sorted(votesByQuery.items(), key=lambda x:x[1], reverse=True) \n",
    "        votesAccumulated.append([vote  for vote,_ in sorted_votes])\n",
    "    return np.array(votesAccumulated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_top_late(d1: np.array, d2: np.array, d3: np.array, simfunction, idx_values, batches:int = 1, top: int=100):\n",
    "    \n",
    "    sx, sy = d1.shape\n",
    "    splits_1 = np.array_split(d1, batches, axis=0)\n",
    "    splits_2 = np.array_split(d2, batches, axis=0)\n",
    "    splits_3 = np.array_split(d3, batches, axis=0)\n",
    "    splits_idx = np.array_split(np.arange(sx), batches, axis=0)\n",
    "\n",
    "    \n",
    "\n",
    "    top_values = []\n",
    "    for b1,b2,b3,i in tqdm(list(zip(splits_1, splits_2, splits_3, splits_idx))):\n",
    "        size_batch = b1.shape[0]\n",
    "        # Calculate similarities\n",
    "        result1 = simfunction(d1, b1).T\n",
    "        result2 = simfunction(d2, b2).T\n",
    "        result3 = simfunction(d3, b3).T\n",
    "    \n",
    "        # Normalize similarities by magnitude      \n",
    "#         n1 = np.linalg.norm(result1 , axis=1, keepdims=True)\n",
    "#         n2 = np.linalg.norm(result2 , axis=1, keepdims=True)\n",
    "#         n3 = np.linalg.norm(result3 , axis=1, keepdims=True)\n",
    "#         result1 =  np.divide(result1, n1, out=np.zeros_like(result1), where=n1!=0)\n",
    "#         result2 =  np.divide(result2, n2, out=np.zeros_like(result2), where=n2!=0)\n",
    "#         result3 =  np.divide(result3, n3, out=np.zeros_like(result3), where=n3!=0)\n",
    "\n",
    "#         # Standardize similarities \n",
    "#         result1 = ((result1.T - np.mean(result1, axis=1))/np.std(result1, axis=1)).T\n",
    "#         result2 = ((result2.T - np.mean(result2, axis=1))/np.std(result2, axis=1)).T\n",
    "#         result3 = ((result3.T - np.mean(result3, axis=1))/np.std(result3, axis=1)).T\n",
    "       \n",
    "        \n",
    "        result1[(np.arange(size_batch),i)] = -1\n",
    "        result2[(np.arange(size_batch),i)] = -1\n",
    "        result3[(np.arange(size_batch),i)] = -1\n",
    "\n",
    "        votes1 = idx_values[np.argsort(result1 * -1, axis=1)]\n",
    "        votes2 = idx_values[np.argsort(result2 * -1, axis=1)]\n",
    "        votes3 = idx_values[np.argsort(result3 * -1, axis=1)]\n",
    "        \n",
    "        # Borda Count Voting\n",
    "        \n",
    "        v1 = np.apply_along_axis(assign_vote_values,1, votes1)\n",
    "        v2 = np.apply_along_axis(assign_vote_values,1, votes2)\n",
    "        v3 = np.apply_along_axis(assign_vote_values,1, votes3)\n",
    "        \n",
    "        result_voting = accumulate_votes(v1,v2,v3)\n",
    "        \n",
    "        top_values.append(result_voting[:,:top])\n",
    "\n",
    "    return np.concatenate(top_values, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 150/150 [3:13:55<00:00, 77.57s/it]\n"
     ]
    }
   ],
   "source": [
    "indexes = datasets[\"tfidf\"].index.values\n",
    "tops = compute_top_late(\n",
    "        datasets[\"tfidf\"].loc[indexes].to_numpy(dtype=np.float32), \n",
    "        datasets[\"mfcc_bow\"].loc[indexes].to_numpy(dtype=np.float32), \n",
    "        datasets[\"incp\"].loc[indexes].to_numpy(dtype=np.float32), \n",
    "        get_cosine_similarity, \n",
    "        indexes, \n",
    "        batches=150,   \n",
    "        top=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.Frame(pd.DataFrame(tops , index=indexes).reset_index()).to_csv('./data/top_ids_cosine_latefusion_borda.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (new)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
